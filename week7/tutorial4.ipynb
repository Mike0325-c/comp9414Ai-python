{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\jjy\\AppData\\Local\\Temp\\ipykernel_15780\\229556668.py\", line 13, in <cell line: 13>\n",
      "    from keras.utils import to_categorical\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\keras\\__init__.py\", line 21, in <module>\n",
      "    from tensorflow.python import tf2\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\tensorflow\\__init__.py\", line 37, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\tensorflow\\python\\__init__.py\", line 37, in <module>\n",
      "    from tensorflow.python.eager import context\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\", line 29, in <module>\n",
      "    from tensorflow.core.framework import function_pb2\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\tensorflow\\core\\framework\\function_pb2.py\", line 16, in <module>\n",
      "    from tensorflow.core.framework import attr_value_pb2 as tensorflow_dot_core_dot_framework_dot_attr__value__pb2\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\tensorflow\\core\\framework\\attr_value_pb2.py\", line 16, in <module>\n",
      "    from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py\", line 16, in <module>\n",
      "    from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py\", line 16, in <module>\n",
      "    from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py\", line 36, in <module>\n",
      "    _descriptor.FieldDescriptor(\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 553, in __new__\n",
      "    _message.Message._CheckCalledFromGeneratedFile()\n",
      "TypeError: Descriptors cannot be created directly.\n",
      "If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n",
      "If you cannot immediately regenerate your protos, some other possible workarounds are:\n",
      " 1. Downgrade the protobuf package to 3.20.x or lower.\n",
      " 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n",
      "\n",
      "More information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\executing\\executing.py\", line 312, in executing\n",
      "    args = executing_cache[key]\n",
      "KeyError: (<code object run_code at 0x000001AE4F5BF9F0, file \"e:\\python3.10\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3362>, 1848167365104, 76)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 799, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 854, in get_records\n",
      "    return list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\stack_data\\core.py\", line 565, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\stack_data\\utils.py\", line 84, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\stack_data\\core.py\", line 555, in mapper\n",
      "    return cls(f, options)\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\stack_data\\core.py\", line 520, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\executing\\executing.py\", line 364, in executing\n",
      "    args = find(source=cls.for_frame(frame), retry_cache=True)\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\executing\\executing.py\", line 247, in for_frame\n",
      "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\executing\\executing.py\", line 265, in for_filename\n",
      "    result = source_cache[filename] = cls._for_filename_and_lines(filename, lines)\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\executing\\executing.py\", line 276, in _for_filename_and_lines\n",
      "    result = source_cache[(filename, lines)] = cls(filename, lines)\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\stack_data\\core.py\", line 79, in __init__\n",
      "    super(Source, self).__init__(*args, **kwargs)\n",
      "  File \"e:\\python3.10\\lib\\site-packages\\executing\\executing.py\", line 228, in __init__\n",
      "    self.tree = ast.parse(ast_text, filename=filename)\n",
      "  File \"e:\\python3.10\\lib\\ast.py\", line 50, in parse\n",
      "    return compile(source, filename, mode, flags,\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Jun 11 13:51:04 2023\n",
    "\n",
    "@author: Maryam Hashemi\n",
    "\"\"\"\n",
    "############################### 5.1 Exercise #####################################\n",
    "\n",
    "############################ 5.1.2 ####################################\n",
    "\n",
    "\n",
    "# import tensorflow_datasets as tfds\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "# import cv2\n",
    "\n",
    "\n",
    "\n",
    "## Loading MNIST dataset\n",
    "(train_ds, train_labels), (test_ds, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "\n",
    "## MNIST dataset images are 28*28, and they are between 0-255. \n",
    "#Preprocessing part 1 \n",
    "train_ds = train_ds / 255.0\n",
    "test_ds = test_ds / 255.0\n",
    "\n",
    "## Transforming labels\n",
    "train_labels = to_categorical(train_labels, num_classes=10)\n",
    "test_labels = to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "\n",
    "\n",
    "############################ 5.1.3 ####################################\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "## Loading VGG16 model\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(32, 32, 3))\n",
    "base_model.trainable = False ## Not trainable weights\n",
    "\n",
    "## Preprocessing part 2\n",
    "train_ds = tf.image.grayscale_to_rgb(tf.expand_dims(train_ds, -1))\n",
    "train_ds = tf.image.resize(train_ds, (32, 32))\n",
    "train_ds = preprocess_input(train_ds)\n",
    "\n",
    "test_ds = tf.image.grayscale_to_rgb(tf.expand_dims(test_ds, -1))\n",
    "test_ds = tf.image.resize(test_ds, (32, 32))\n",
    "test_ds = preprocess_input(test_ds)\n",
    "\n",
    "\n",
    "\n",
    "############################ 5.1.4 ####################################\n",
    "\n",
    "base_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "############################ 5.1.5 ####################################\n",
    "\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "flatten_layer = layers.Flatten()\n",
    "dense_layer_1 = layers.Dense(50, activation='relu')\n",
    "dense_layer_2 = layers.Dense(20, activation='relu')\n",
    "prediction_layer = layers.Dense(10, activation='softmax')\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5, restore_best_weights=True)\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "history = model.fit(train_ds, train_labels, epochs=5, validation_split=0.2, batch_size=32, callbacks=[es])\n",
    "\n",
    "score = model.evaluate(test_ds, test_labels, batch_size=32)\n",
    "x_valid_output_images = model.predict(test_ds)\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "\n",
    "############################ 5.1.6 ####################################\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, history.history['loss'], 'r--')\n",
    "plt.plot(epoch_count, history.history['val_loss'], 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "elapsed = end - start\n",
    "print('Training time:', str(elapsed))\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "############################ 5.1.7 ####################################\n",
    "\n",
    "\n",
    "## Loading MNIST dataset\n",
    "(train_ds, train_labels), (test_ds, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "## Displaying an image\n",
    "image_index = 10\n",
    "plt.imshow(train_ds[image_index], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "## Rescaling images\n",
    "train_ds = train_ds / 255.0\n",
    "test_ds = test_ds / 255.0\n",
    "\n",
    "## Remaining code for model training and evaluation\n",
    "# (Please copy the code provided in the previous response here)\n",
    "\n",
    "# Use the image for prediction\n",
    "input_image = tf.image.grayscale_to_rgb(tf.expand_dims(train_ds[image_index], -1))\n",
    "input_image = tf.image.resize(input_image, (32, 32))\n",
    "input_image= preprocess_input(input_image)\n",
    "input_image = tf.expand_dims(input_image, axis=0)\n",
    "\n",
    "\n",
    "## Transforming labels to correct format\n",
    "train_labels = to_categorical(train_labels, num_classes=10)\n",
    "test_labels = to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "predicted_label = model.predict(input_image)\n",
    "predicted_class = tf.argmax(predicted_label, axis=1).numpy()[0]\n",
    "\n",
    "actual_class = train_labels[image_index]\n",
    "\n",
    "print(\"Actual class:\", actual_class)\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
